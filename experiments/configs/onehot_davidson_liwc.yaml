# File: onehot_davidson_liwc.yaml

# Storage, GPU, etc.
datadir: data/
save_model: results/models/
results: results/
gpu: 1
shuffle: True
tokenizer: ekphrasis

# Experiment
train: davidson
experiment: liwc
cleaners: [username, hashtag, url, lower]

# Model
model: [mlp, cnn, rnn, lstm]
encoding: onehot
optimizer: adam
loss: nlll
seed: 42
nonlinearity: [tanh, relu]

# Metrics, earlyss topping, etc.
metrics: [accuracy, precision, recall, f1-score]
display: f1-score
stop_metric: loss
patience: 15

# Hyper parameters
hyperparams: [batch_size, epochs, learning_rate, dropout, embedding, hidden, nonlinearity, filters, window_sizes]
epochs: [50, 100, 200]
batch_size: [16, 32, 64]
learning_rate: 
  high: 1.0
  low: 1e-5
dropout:
  high: 0.5
  low: 0.0
embedding: [100, 200, 300]
hidden: [100, 200, 300]

# Model parameters
layers: 1
window_sizes: [[2, 3, 4], [1, 2, 3], [3, 4, 5]]
filters: [64, 128, 256]
