# File: embedding_wulczyn_liwc.yaml
# Sets up experiments for LIWC vocab reduction using the Davidson et al dataset as the training dataset
train: wulczyn
model: [all]
experiment: liwc
cleaners: [username, hashtag, url, lower]
metrics: [accuracy, precision, recall, f1-score]
stop_metric: loss
patience: 15
display: f1-score
optimizer: adam
loss: nlll
encoding: embedding
layers: 1
window_sizes: [[1, 2, 3], [2, 3, 4], [3, 4, 5]]
filters: [64, 128]
embedding: [100, 200, 300]
hidden: [32, 64, 100, 300]
batch_size: [16, 32, 64]
dropout: [0.0, 0.1, 0.2]
learning_rate: [0.1, 0.01, 0.001, 0.0001]
epochs: [50, 100, 200]
activation: [tanh, relu]
seed: 42
gpu: True
shuffle: False
