dataset	epoch	model	input dim	hidden dim	embedding dim	dropout	learning rate	window sizes	num filters	max feats	output dim	accuracy	train loss	dev accuracy	dev loss
Davidson et al.	0	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.7149529569892473	59815.2342133522	0.8259850543478262	775.9471817016602
Davidson et al.	1	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.691093661573288	153514.9334564209	0.8259850543478262	2453.738525390625
Davidson et al.	2	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6842724250141483	297447.7653684616	0.17401494565217393	3955.642919921875
Davidson et al.	3	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6651536856253537	377725.2801132202	0.8259850543478262	4170.975109863281
Davidson et al.	4	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.830136707696661	529152.4625244141	0.8259850543478262	1670.1147857666015
Davidson et al.	5	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6527323853989814	181199.28741943836	0.17401494565217393	2124.9813842773438
Davidson et al.	6	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6984896717600453	245236.56935358047	0.8259850543478262	2855.851397705078
Davidson et al.	7	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8301057583474816	314770.39138793945	0.8259850543478262	198.202725982666
Davidson et al.	8	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.7351460809281267	109731.73281943798	0.8259850543478262	1230.1866363525392
Davidson et al.	9	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6066885964912281	165731.2093477249	0.17401494565217393	668.6641662597656
Davidson et al.	10	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6527173528579514	44232.82709658146	0.8259850543478262	572.0912216186523
Davidson et al.	11	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6284433361629881	101908.14522743225	0.8259850543478262	846.7757598876954
Davidson et al.	12	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.5969713851160159	149488.48325538635	0.8259850543478262	166.66211700439453
Davidson et al.	13	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6533160016977928	83715.93799543381	0.8259850543478262	1399.3602172851563
Davidson et al.	14	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6656418010752687	200261.81136131287	0.17401494565217393	2621.692175292969
Davidson et al.	15	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6872815860215054	303255.89500951767	0.8259850543478262	3581.4159973144533
Davidson et al.	16	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8301243279569891	423131.45251464844	0.8259850543478262	832.6010757446289
Davidson et al.	17	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6471482385398982	84942.62227630615	0.8259850543478262	1295.4000091552734
Davidson et al.	18	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6657983163554047	189098.56490516663	0.17401494565217393	2487.1186767578124
Davidson et al.	19	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6856112054329372	290531.4252090454	0.8259850543478262	3402.710559082031
Davidson et al.	20	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8301057583474816	400959.6763305664	0.8259850543478262	787.7516677856445
Davidson et al.	21	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6548280984719864	79792.65609025955	0.8259850543478262	1243.298583984375
Davidson et al.	22	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6679541595925298	180284.26038265228	0.17401494565217393	2410.4123046875
Davidson et al.	23	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6853662634408602	281488.29623031616	0.8259850543478262	3317.974743652344
Davidson et al.	24	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8300995684776455	389336.53924560547	0.8259850543478262	756.285269165039
Davidson et al.	25	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6574304966044143	78255.70296096802	0.8259850543478262	1262.400131225586
Davidson et al.	26	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6719041808149405	177930.06955242157	0.17401494565217393	2413.5533813476563
Davidson et al.	27	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6847738044708546	278854.38200616837	0.8259850543478262	3264.241143798828
Davidson et al.	28	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8301057583474816	386295.7063598633	0.8259850543478262	708.7330337524414
Davidson et al.	29	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6610719086021506	76259.17122387886	0.8259850543478262	1262.3235778808594
Davidson et al.	30	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6566001697792869	184941.3883228302	0.17401494565217393	2290.4068298339844
Davidson et al.	31	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6891845288624787	269690.4164953232	0.8259850543478262	3151.905133056641
Davidson et al.	32	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8301119482173176	364354.6225891113	0.8259850543478262	581.0449584960937
Davidson et al.	33	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6857447297679684	73373.00268113613	0.8259850543478262	1313.6387023925781
Davidson et al.	34	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.653948252688172	182078.19808006287	0.17401494565217393	2185.3042846679687
Davidson et al.	35	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6945334606677985	257814.1414823532	0.8259850543478262	2948.530694580078
Davidson et al.	36	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8301057583474816	325715.3846130371	0.8259850543478262	313.37301483154295
Davidson et al.	37	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.7455990025466893	69267.82393097878	0.8259850543478262	698.5917037963867
Davidson et al.	38	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6391208616298812	89566.73002529144	0.8259850543478262	1115.3891983032227
Davidson et al.	39	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6324826683644595	182131.7417163849	0.17401494565217393	1683.134295654297
Davidson et al.	40	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.7292993067345783	191693.75602054596	0.8259850543478262	2016.6584075927735
Davidson et al.	41	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.7272097835314091	216179.04230451584	0.17401494565217393	3146.2939697265624
Davidson et al.	42	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6530383418222977	363007.8249797821	0.8259850543478262	4266.415808105468
Davidson et al.	43	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8301305178268251	553293.8365478516	0.8259850543478262	1874.0100524902343
Davidson et al.	44	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6886380517826824	185971.1742553711	0.17401494565217393	2610.3964599609376
Davidson et al.	45	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6718210597057158	297682.1065325737	0.8259850543478262	3527.1371215820313
Davidson et al.	46	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8301305178268251	438883.3868408203	0.8259850543478262	1217.661312866211
Davidson et al.	47	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.5962533602150538	127684.36132383347	0.8259850543478262	130.89530830383302
Davidson et al.	48	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.652540499434069	70369.19124484062	0.8259850543478262	1230.0737182617188
Davidson et al.	49	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.674090973401245	174507.4043188095	0.17401494565217393	2438.0844970703124
Davidson et al.	50	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6789146505376343	277542.721347332	0.8259850543478262	3315.946856689453
Davidson et al.	51	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.8300500495189588	400027.17822265625	0.8259850543478262	888.7047683715821
Davidson et al.	52	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6328973896434635	84464.2084145546	0.8259850543478262	905.4229782104492
Davidson et al.	53	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6246197651386531	155198.76331329346	0.17401494565217393	1230.1204528808594
Davidson et al.	54	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.7485904782116581	127304.7175192833	0.8259850543478262	1043.9714080810547
Davidson et al.	55	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6067619906621393	125758.22408103943	0.8259850543478262	363.68413162231445
Davidson et al.	56	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6329097693831353	91763.1753988266	0.8259850543478262	949.8665557861328
Davidson et al.	57	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.6177773061686475	163883.5816268921	0.17401494565217393	946.8185455322266
Davidson et al.	58	mlp	95	300	300	0.2	0.2	[2, 3, 4]	120	100	2	0.7340708121109225	59201.989251732826	0.17401494565217393	919.5370330810547
